global:
  output_dir: "./results"
  log_level: "info"
  random_seed: 42
  max_tokens: 512
  lora_path: None


gsm8k:
  model: "gpt-4"
  lora_path: None
  n_shots: 1
  n_problems: 100
  enable_cot: False
  temperature: 0
  sampling_params:
    temperature: 0.7
    max_tokens: 512


mmlu:
  temperature:1
  model: "gpt-4"
  max_tokens: 512
  sampling_params:
    temperature: 0.7
    max_tokens: 512
  few_shot_examples: 5
  lora_path: None
  n_shots: 1
  batch_size: 32


humaneval:
  model: "gpt-4"
  sampling_params:
    temperature: 0.7
    max_tokens: 512
  num_samples_per_task: 10
  lora_path: None
  k: 5

mbpp:
  model: "gpt-4"
  num_samples: 150
  sampling_params:
    temperature: 0.7
    max_tokens: 512

drop:
  model: "gpt-3.5-turbo"
  max_tokens: 512
  num_beams: 5
  lora_path: None
  sampling_params:
    max_tokens: 512

hellaswag:
  num_choices: 5
  max_context_length: 2048
  batch_size: 32
  max_tokens: 512
  max_seq_length: 128
  lora_path: None
  sampling_params:
    temperature: 0.7
    max_tokens: 512


custom_benchmark:
  model: "gpt-4"
  name: "my_custom_test"
  parameters:
    param1: "value1"
    param2: 42
  sampling_params:
    temperature: 0.7
    max_tokens: 512